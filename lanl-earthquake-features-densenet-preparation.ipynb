{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils import shuffle\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "n_cpu = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "20d2180d96fe3b16f0992dd4ad6e695ae27150c6"
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"../input/features_scaled.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "051275c1e4b89051346dea2208ec8da0cc9a1642"
   },
   "outputs": [],
   "source": [
    "Y = pd.read_csv(\"../input/logits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "fce1dc6a3ac2227e6878d8992764cc38d1c87af5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>Rmean</th>\n",
       "      <th>Rstd</th>\n",
       "      <th>Rmax</th>\n",
       "      <th>Rmin</th>\n",
       "      <th>Imean</th>\n",
       "      <th>Istd</th>\n",
       "      <th>Imax</th>\n",
       "      <th>Imin</th>\n",
       "      <th>Rmean_last_5000</th>\n",
       "      <th>Rstd__last_5000</th>\n",
       "      <th>Rmax_last_5000</th>\n",
       "      <th>Rmin_last_5000</th>\n",
       "      <th>Rmean_last_15000</th>\n",
       "      <th>Rstd_last_15000</th>\n",
       "      <th>Rmax_last_15000</th>\n",
       "      <th>Rmin_last_15000</th>\n",
       "      <th>mean_change_abs</th>\n",
       "      <th>mean_change_rate</th>\n",
       "      <th>abs_max</th>\n",
       "      <th>abs_min</th>\n",
       "      <th>std_first_50000</th>\n",
       "      <th>std_last_50000</th>\n",
       "      <th>std_first_10000</th>\n",
       "      <th>std_last_10000</th>\n",
       "      <th>avg_first_50000</th>\n",
       "      <th>avg_last_50000</th>\n",
       "      <th>avg_first_10000</th>\n",
       "      <th>avg_last_10000</th>\n",
       "      <th>min_first_50000</th>\n",
       "      <th>min_last_50000</th>\n",
       "      <th>min_first_10000</th>\n",
       "      <th>min_last_10000</th>\n",
       "      <th>max_first_50000</th>\n",
       "      <th>max_last_50000</th>\n",
       "      <th>max_first_10000</th>\n",
       "      <th>...</th>\n",
       "      <th>q01_roll_std_100</th>\n",
       "      <th>q05_roll_std_100</th>\n",
       "      <th>q95_roll_std_100</th>\n",
       "      <th>q99_roll_std_100</th>\n",
       "      <th>av_change_abs_roll_std_100</th>\n",
       "      <th>av_change_rate_roll_std_100</th>\n",
       "      <th>abs_max_roll_std_100</th>\n",
       "      <th>ave_roll_mean_100</th>\n",
       "      <th>std_roll_mean_100</th>\n",
       "      <th>max_roll_mean_100</th>\n",
       "      <th>min_roll_mean_100</th>\n",
       "      <th>q01_roll_mean_100</th>\n",
       "      <th>q05_roll_mean_100</th>\n",
       "      <th>q95_roll_mean_100</th>\n",
       "      <th>q99_roll_mean_100</th>\n",
       "      <th>av_change_abs_roll_mean_100</th>\n",
       "      <th>av_change_rate_roll_mean_100</th>\n",
       "      <th>abs_max_roll_mean_100</th>\n",
       "      <th>ave_roll_std_1000</th>\n",
       "      <th>std_roll_std_1000</th>\n",
       "      <th>max_roll_std_1000</th>\n",
       "      <th>min_roll_std_1000</th>\n",
       "      <th>q01_roll_std_1000</th>\n",
       "      <th>q05_roll_std_1000</th>\n",
       "      <th>q95_roll_std_1000</th>\n",
       "      <th>q99_roll_std_1000</th>\n",
       "      <th>av_change_abs_roll_std_1000</th>\n",
       "      <th>av_change_rate_roll_std_1000</th>\n",
       "      <th>abs_max_roll_std_1000</th>\n",
       "      <th>ave_roll_mean_1000</th>\n",
       "      <th>std_roll_mean_1000</th>\n",
       "      <th>max_roll_mean_1000</th>\n",
       "      <th>min_roll_mean_1000</th>\n",
       "      <th>q01_roll_mean_1000</th>\n",
       "      <th>q05_roll_mean_1000</th>\n",
       "      <th>q95_roll_mean_1000</th>\n",
       "      <th>q99_roll_mean_1000</th>\n",
       "      <th>av_change_abs_roll_mean_1000</th>\n",
       "      <th>av_change_rate_roll_mean_1000</th>\n",
       "      <th>abs_max_roll_mean_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.424140</td>\n",
       "      <td>-0.170214</td>\n",
       "      <td>-0.218194</td>\n",
       "      <td>0.193218</td>\n",
       "      <td>1.199316</td>\n",
       "      <td>-0.105007</td>\n",
       "      <td>1.424140</td>\n",
       "      <td>0.156736</td>\n",
       "      <td>-0.458570</td>\n",
       "      <td>-0.169052</td>\n",
       "      <td>-0.085419</td>\n",
       "      <td>0.085419</td>\n",
       "      <td>-0.050936</td>\n",
       "      <td>-0.191893</td>\n",
       "      <td>-0.205689</td>\n",
       "      <td>0.051904</td>\n",
       "      <td>0.234333</td>\n",
       "      <td>-0.171669</td>\n",
       "      <td>-0.117546</td>\n",
       "      <td>0.156736</td>\n",
       "      <td>-1.326420</td>\n",
       "      <td>-1.569265</td>\n",
       "      <td>-0.222567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052067</td>\n",
       "      <td>-0.285579</td>\n",
       "      <td>0.724223</td>\n",
       "      <td>-0.192422</td>\n",
       "      <td>1.601861</td>\n",
       "      <td>0.367222</td>\n",
       "      <td>2.169234</td>\n",
       "      <td>0.502224</td>\n",
       "      <td>-0.042513</td>\n",
       "      <td>0.465851</td>\n",
       "      <td>-0.563306</td>\n",
       "      <td>0.406807</td>\n",
       "      <td>0.011907</td>\n",
       "      <td>-0.400167</td>\n",
       "      <td>0.718612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087853</td>\n",
       "      <td>0.138030</td>\n",
       "      <td>-0.252853</td>\n",
       "      <td>-0.188298</td>\n",
       "      <td>-0.125616</td>\n",
       "      <td>-1.418309</td>\n",
       "      <td>-0.185996</td>\n",
       "      <td>1.423148</td>\n",
       "      <td>-0.098604</td>\n",
       "      <td>-0.153539</td>\n",
       "      <td>0.241431</td>\n",
       "      <td>0.305730</td>\n",
       "      <td>1.053727</td>\n",
       "      <td>1.245910</td>\n",
       "      <td>0.155664</td>\n",
       "      <td>-0.487149</td>\n",
       "      <td>-1.392181</td>\n",
       "      <td>-0.153194</td>\n",
       "      <td>-0.164639</td>\n",
       "      <td>-0.187411</td>\n",
       "      <td>-0.112163</td>\n",
       "      <td>0.286995</td>\n",
       "      <td>0.102309</td>\n",
       "      <td>0.184599</td>\n",
       "      <td>-0.431605</td>\n",
       "      <td>-0.208502</td>\n",
       "      <td>-0.296462</td>\n",
       "      <td>-1.823622</td>\n",
       "      <td>-0.112163</td>\n",
       "      <td>1.421036</td>\n",
       "      <td>0.268470</td>\n",
       "      <td>-0.004742</td>\n",
       "      <td>0.178278</td>\n",
       "      <td>0.287332</td>\n",
       "      <td>0.965402</td>\n",
       "      <td>1.509153</td>\n",
       "      <td>0.885262</td>\n",
       "      <td>-0.631300</td>\n",
       "      <td>-1.832422</td>\n",
       "      <td>-0.004742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.805716</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>0.063936</td>\n",
       "      <td>-0.018037</td>\n",
       "      <td>0.078886</td>\n",
       "      <td>-0.008599</td>\n",
       "      <td>0.805716</td>\n",
       "      <td>-0.222118</td>\n",
       "      <td>0.286705</td>\n",
       "      <td>0.007212</td>\n",
       "      <td>0.042859</td>\n",
       "      <td>-0.042859</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>-0.105219</td>\n",
       "      <td>0.097303</td>\n",
       "      <td>0.421938</td>\n",
       "      <td>0.014326</td>\n",
       "      <td>0.137448</td>\n",
       "      <td>-0.222118</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>-1.040206</td>\n",
       "      <td>0.036797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153858</td>\n",
       "      <td>-0.076987</td>\n",
       "      <td>-0.195869</td>\n",
       "      <td>-0.310470</td>\n",
       "      <td>0.648209</td>\n",
       "      <td>0.978585</td>\n",
       "      <td>0.845958</td>\n",
       "      <td>1.270320</td>\n",
       "      <td>-0.384878</td>\n",
       "      <td>-0.140002</td>\n",
       "      <td>0.197634</td>\n",
       "      <td>0.282559</td>\n",
       "      <td>0.541182</td>\n",
       "      <td>0.026977</td>\n",
       "      <td>-0.316481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053418</td>\n",
       "      <td>0.141753</td>\n",
       "      <td>0.031138</td>\n",
       "      <td>-0.043328</td>\n",
       "      <td>-0.015101</td>\n",
       "      <td>-1.107457</td>\n",
       "      <td>0.086114</td>\n",
       "      <td>0.805142</td>\n",
       "      <td>-0.046104</td>\n",
       "      <td>0.115951</td>\n",
       "      <td>0.005951</td>\n",
       "      <td>0.198170</td>\n",
       "      <td>0.697867</td>\n",
       "      <td>0.621246</td>\n",
       "      <td>0.022997</td>\n",
       "      <td>-0.137008</td>\n",
       "      <td>-1.097765</td>\n",
       "      <td>0.112852</td>\n",
       "      <td>0.087108</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>-0.022923</td>\n",
       "      <td>-0.143446</td>\n",
       "      <td>0.050644</td>\n",
       "      <td>-0.151082</td>\n",
       "      <td>-0.048382</td>\n",
       "      <td>0.148106</td>\n",
       "      <td>-0.003689</td>\n",
       "      <td>-0.895105</td>\n",
       "      <td>-0.022923</td>\n",
       "      <td>0.802052</td>\n",
       "      <td>-0.141264</td>\n",
       "      <td>0.007341</td>\n",
       "      <td>-0.025387</td>\n",
       "      <td>0.622391</td>\n",
       "      <td>0.842747</td>\n",
       "      <td>0.522428</td>\n",
       "      <td>0.294357</td>\n",
       "      <td>-0.912054</td>\n",
       "      <td>-0.890022</td>\n",
       "      <td>0.007341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.511155</td>\n",
       "      <td>0.049252</td>\n",
       "      <td>-0.086289</td>\n",
       "      <td>0.163039</td>\n",
       "      <td>0.078886</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>1.511155</td>\n",
       "      <td>-0.104254</td>\n",
       "      <td>-0.540244</td>\n",
       "      <td>0.054839</td>\n",
       "      <td>0.138553</td>\n",
       "      <td>-0.138553</td>\n",
       "      <td>1.008297</td>\n",
       "      <td>0.057049</td>\n",
       "      <td>0.112694</td>\n",
       "      <td>-0.082458</td>\n",
       "      <td>-0.191007</td>\n",
       "      <td>0.057127</td>\n",
       "      <td>0.094468</td>\n",
       "      <td>-0.104254</td>\n",
       "      <td>-0.218781</td>\n",
       "      <td>0.949925</td>\n",
       "      <td>-0.101306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.277885</td>\n",
       "      <td>0.373967</td>\n",
       "      <td>0.681222</td>\n",
       "      <td>0.676055</td>\n",
       "      <td>2.067734</td>\n",
       "      <td>0.532791</td>\n",
       "      <td>2.128155</td>\n",
       "      <td>0.024737</td>\n",
       "      <td>-0.081995</td>\n",
       "      <td>-0.451701</td>\n",
       "      <td>-0.447401</td>\n",
       "      <td>-0.036209</td>\n",
       "      <td>0.179905</td>\n",
       "      <td>0.622003</td>\n",
       "      <td>...</td>\n",
       "      <td>1.143455</td>\n",
       "      <td>0.856220</td>\n",
       "      <td>0.666432</td>\n",
       "      <td>0.071185</td>\n",
       "      <td>0.121662</td>\n",
       "      <td>0.526623</td>\n",
       "      <td>-0.178218</td>\n",
       "      <td>1.509946</td>\n",
       "      <td>-0.005550</td>\n",
       "      <td>-0.058425</td>\n",
       "      <td>0.122544</td>\n",
       "      <td>0.204497</td>\n",
       "      <td>0.989025</td>\n",
       "      <td>1.508926</td>\n",
       "      <td>0.361297</td>\n",
       "      <td>1.081484</td>\n",
       "      <td>0.516322</td>\n",
       "      <td>-0.059295</td>\n",
       "      <td>0.350030</td>\n",
       "      <td>0.007162</td>\n",
       "      <td>-0.061793</td>\n",
       "      <td>1.038796</td>\n",
       "      <td>0.732837</td>\n",
       "      <td>0.769589</td>\n",
       "      <td>0.650231</td>\n",
       "      <td>-0.038828</td>\n",
       "      <td>0.064478</td>\n",
       "      <td>0.635232</td>\n",
       "      <td>-0.061793</td>\n",
       "      <td>1.508574</td>\n",
       "      <td>0.085078</td>\n",
       "      <td>0.099556</td>\n",
       "      <td>0.245184</td>\n",
       "      <td>0.634878</td>\n",
       "      <td>1.207106</td>\n",
       "      <td>1.530919</td>\n",
       "      <td>0.889790</td>\n",
       "      <td>0.441128</td>\n",
       "      <td>0.639209</td>\n",
       "      <td>0.099556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.494934</td>\n",
       "      <td>0.043950</td>\n",
       "      <td>0.122560</td>\n",
       "      <td>-0.187796</td>\n",
       "      <td>0.078886</td>\n",
       "      <td>0.044798</td>\n",
       "      <td>1.494934</td>\n",
       "      <td>-0.033851</td>\n",
       "      <td>-0.417733</td>\n",
       "      <td>0.042217</td>\n",
       "      <td>0.047721</td>\n",
       "      <td>-0.047721</td>\n",
       "      <td>0.164562</td>\n",
       "      <td>0.187645</td>\n",
       "      <td>0.261908</td>\n",
       "      <td>-0.270646</td>\n",
       "      <td>-0.979372</td>\n",
       "      <td>0.060224</td>\n",
       "      <td>0.093069</td>\n",
       "      <td>-0.033851</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>-0.634909</td>\n",
       "      <td>0.097427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020852</td>\n",
       "      <td>-0.058805</td>\n",
       "      <td>0.171820</td>\n",
       "      <td>-0.191908</td>\n",
       "      <td>1.173459</td>\n",
       "      <td>1.871230</td>\n",
       "      <td>1.693273</td>\n",
       "      <td>2.100493</td>\n",
       "      <td>0.171465</td>\n",
       "      <td>0.092027</td>\n",
       "      <td>-0.056013</td>\n",
       "      <td>0.096186</td>\n",
       "      <td>-0.187431</td>\n",
       "      <td>-0.115405</td>\n",
       "      <td>0.318376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497101</td>\n",
       "      <td>0.375361</td>\n",
       "      <td>0.124566</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>-0.205746</td>\n",
       "      <td>-0.660198</td>\n",
       "      <td>0.208466</td>\n",
       "      <td>1.493770</td>\n",
       "      <td>0.056268</td>\n",
       "      <td>0.180579</td>\n",
       "      <td>-0.236414</td>\n",
       "      <td>0.274095</td>\n",
       "      <td>1.053727</td>\n",
       "      <td>1.344541</td>\n",
       "      <td>0.248530</td>\n",
       "      <td>-0.094991</td>\n",
       "      <td>-0.607145</td>\n",
       "      <td>0.176655</td>\n",
       "      <td>0.130568</td>\n",
       "      <td>0.063349</td>\n",
       "      <td>0.143073</td>\n",
       "      <td>0.089291</td>\n",
       "      <td>0.229149</td>\n",
       "      <td>0.296334</td>\n",
       "      <td>0.141775</td>\n",
       "      <td>-0.076650</td>\n",
       "      <td>-0.079737</td>\n",
       "      <td>-1.102767</td>\n",
       "      <td>0.143073</td>\n",
       "      <td>1.491578</td>\n",
       "      <td>0.083085</td>\n",
       "      <td>0.068076</td>\n",
       "      <td>0.105059</td>\n",
       "      <td>0.770151</td>\n",
       "      <td>1.160208</td>\n",
       "      <td>1.432972</td>\n",
       "      <td>0.815078</td>\n",
       "      <td>-0.949994</td>\n",
       "      <td>-1.097513</td>\n",
       "      <td>0.068076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.520242</td>\n",
       "      <td>0.088495</td>\n",
       "      <td>-0.067969</td>\n",
       "      <td>0.087590</td>\n",
       "      <td>1.199316</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>1.520242</td>\n",
       "      <td>-0.056324</td>\n",
       "      <td>0.113148</td>\n",
       "      <td>0.086842</td>\n",
       "      <td>0.051860</td>\n",
       "      <td>-0.051860</td>\n",
       "      <td>-0.269909</td>\n",
       "      <td>0.327596</td>\n",
       "      <td>0.538636</td>\n",
       "      <td>-0.520145</td>\n",
       "      <td>0.597382</td>\n",
       "      <td>0.106941</td>\n",
       "      <td>0.077312</td>\n",
       "      <td>-0.056324</td>\n",
       "      <td>-0.108017</td>\n",
       "      <td>0.094279</td>\n",
       "      <td>-0.084464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.093104</td>\n",
       "      <td>0.174161</td>\n",
       "      <td>-0.044712</td>\n",
       "      <td>1.006489</td>\n",
       "      <td>1.345312</td>\n",
       "      <td>1.126873</td>\n",
       "      <td>1.379779</td>\n",
       "      <td>0.608046</td>\n",
       "      <td>-0.030285</td>\n",
       "      <td>-0.210900</td>\n",
       "      <td>-0.015429</td>\n",
       "      <td>-1.301610</td>\n",
       "      <td>-0.063704</td>\n",
       "      <td>0.116624</td>\n",
       "      <td>0.069954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265267</td>\n",
       "      <td>0.319221</td>\n",
       "      <td>0.480216</td>\n",
       "      <td>0.207497</td>\n",
       "      <td>0.031992</td>\n",
       "      <td>0.555235</td>\n",
       "      <td>-0.017061</td>\n",
       "      <td>1.522001</td>\n",
       "      <td>0.037614</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>0.103264</td>\n",
       "      <td>0.166535</td>\n",
       "      <td>1.215482</td>\n",
       "      <td>1.410295</td>\n",
       "      <td>0.507230</td>\n",
       "      <td>0.157111</td>\n",
       "      <td>0.551866</td>\n",
       "      <td>0.022164</td>\n",
       "      <td>0.213406</td>\n",
       "      <td>0.103858</td>\n",
       "      <td>0.057825</td>\n",
       "      <td>0.462472</td>\n",
       "      <td>0.282603</td>\n",
       "      <td>0.319527</td>\n",
       "      <td>0.500440</td>\n",
       "      <td>0.115098</td>\n",
       "      <td>0.896770</td>\n",
       "      <td>-0.480335</td>\n",
       "      <td>0.057825</td>\n",
       "      <td>1.525581</td>\n",
       "      <td>-0.164151</td>\n",
       "      <td>0.138032</td>\n",
       "      <td>0.187535</td>\n",
       "      <td>1.040695</td>\n",
       "      <td>1.557034</td>\n",
       "      <td>1.393068</td>\n",
       "      <td>0.901110</td>\n",
       "      <td>0.595416</td>\n",
       "      <td>-0.465464</td>\n",
       "      <td>0.138032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           ...            abs_max_roll_mean_1000\n",
       "0           0           ...                         -0.004742\n",
       "1           1           ...                          0.007341\n",
       "2           2           ...                          0.099556\n",
       "3           3           ...                          0.068076\n",
       "4           4           ...                          0.138032\n",
       "\n",
       "[5 rows x 155 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "54763b895d486dbd46d93ffc9ac779a5185a07a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.430797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.391499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.353196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.313798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.274400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  time_to_failure\n",
       "0           0         1.430797\n",
       "1           1         1.391499\n",
       "2           2         1.353196\n",
       "3           3         1.313798\n",
       "4           4         1.274400"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "f4057d965e72d50c27b6871a8f96c5c22e1beeaf"
   },
   "outputs": [],
   "source": [
    "X = X.iloc[:,1:]\n",
    "Y = Y.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "2a15d091ca0ff25af12675e8baf91c0f98ee5298"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "23ab1e6d482d264ede673f183c8600af94185fb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4194, 154)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "12be5213811e111f0ab240bf2370c600ef5c97b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# n = X.shape[1]\n",
    "n = 2\n",
    "inputs = Input(shape = (154,))\n",
    "x = Dense(n*400, activation = 'relu')(inputs)\n",
    "x = Dense(n*500, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(n*500, activation = 'relu')(x)\n",
    "x = Dense(n*600, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(n*700, activation = 'relu')(x)\n",
    "x = Dense(n*800, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(n*900, activation = 'relu')(x)\n",
    "x = Dense(n*1000, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "outputs = Dense(1, activation = 'linear')(x)\n",
    "model = Model(inputs = inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "9492c85e408cea80fd051d335aefeb809692c863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 154)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 800)               124000    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              801000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1200)              1201200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1200)              4800      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1400)              1681400   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1600)              2241600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1600)              6400      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1800)              2881800   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2000)              3602000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2000)              8000      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 2001      \n",
      "=================================================================\n",
      "Total params: 13,559,201\n",
      "Trainable params: 13,547,601\n",
      "Non-trainable params: 11,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "lr = 5e-4\n",
    "model.compile(optimizer = SGD(lr), loss = \"mae\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "aafce8e4b8332871afbda4af1aed8f4258cc4b60"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.utils import shuffle\n",
    "df = pd.concat([X.reset_index(drop=True), Y], axis=1)\n",
    "df = shuffle(df)\n",
    "X = df.iloc[:,:-1]\n",
    "Y = df.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "8e5c2fc04fee674ecf7eadee69dced7ce03150be"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>Rmean</th>\n",
       "      <th>Rstd</th>\n",
       "      <th>Rmax</th>\n",
       "      <th>Rmin</th>\n",
       "      <th>Imean</th>\n",
       "      <th>Istd</th>\n",
       "      <th>Imax</th>\n",
       "      <th>Imin</th>\n",
       "      <th>Rmean_last_5000</th>\n",
       "      <th>Rstd__last_5000</th>\n",
       "      <th>Rmax_last_5000</th>\n",
       "      <th>Rmin_last_5000</th>\n",
       "      <th>Rmean_last_15000</th>\n",
       "      <th>Rstd_last_15000</th>\n",
       "      <th>Rmax_last_15000</th>\n",
       "      <th>Rmin_last_15000</th>\n",
       "      <th>mean_change_abs</th>\n",
       "      <th>mean_change_rate</th>\n",
       "      <th>abs_max</th>\n",
       "      <th>abs_min</th>\n",
       "      <th>std_first_50000</th>\n",
       "      <th>std_last_50000</th>\n",
       "      <th>std_first_10000</th>\n",
       "      <th>std_last_10000</th>\n",
       "      <th>avg_first_50000</th>\n",
       "      <th>avg_last_50000</th>\n",
       "      <th>avg_first_10000</th>\n",
       "      <th>avg_last_10000</th>\n",
       "      <th>min_first_50000</th>\n",
       "      <th>min_last_50000</th>\n",
       "      <th>min_first_10000</th>\n",
       "      <th>min_last_10000</th>\n",
       "      <th>max_first_50000</th>\n",
       "      <th>max_last_50000</th>\n",
       "      <th>max_first_10000</th>\n",
       "      <th>max_last_10000</th>\n",
       "      <th>...</th>\n",
       "      <th>q01_roll_std_100</th>\n",
       "      <th>q05_roll_std_100</th>\n",
       "      <th>q95_roll_std_100</th>\n",
       "      <th>q99_roll_std_100</th>\n",
       "      <th>av_change_abs_roll_std_100</th>\n",
       "      <th>av_change_rate_roll_std_100</th>\n",
       "      <th>abs_max_roll_std_100</th>\n",
       "      <th>ave_roll_mean_100</th>\n",
       "      <th>std_roll_mean_100</th>\n",
       "      <th>max_roll_mean_100</th>\n",
       "      <th>min_roll_mean_100</th>\n",
       "      <th>q01_roll_mean_100</th>\n",
       "      <th>q05_roll_mean_100</th>\n",
       "      <th>q95_roll_mean_100</th>\n",
       "      <th>q99_roll_mean_100</th>\n",
       "      <th>av_change_abs_roll_mean_100</th>\n",
       "      <th>av_change_rate_roll_mean_100</th>\n",
       "      <th>abs_max_roll_mean_100</th>\n",
       "      <th>ave_roll_std_1000</th>\n",
       "      <th>std_roll_std_1000</th>\n",
       "      <th>max_roll_std_1000</th>\n",
       "      <th>min_roll_std_1000</th>\n",
       "      <th>q01_roll_std_1000</th>\n",
       "      <th>q05_roll_std_1000</th>\n",
       "      <th>q95_roll_std_1000</th>\n",
       "      <th>q99_roll_std_1000</th>\n",
       "      <th>av_change_abs_roll_std_1000</th>\n",
       "      <th>av_change_rate_roll_std_1000</th>\n",
       "      <th>abs_max_roll_std_1000</th>\n",
       "      <th>ave_roll_mean_1000</th>\n",
       "      <th>std_roll_mean_1000</th>\n",
       "      <th>max_roll_mean_1000</th>\n",
       "      <th>min_roll_mean_1000</th>\n",
       "      <th>q01_roll_mean_1000</th>\n",
       "      <th>q05_roll_mean_1000</th>\n",
       "      <th>q95_roll_mean_1000</th>\n",
       "      <th>q99_roll_mean_1000</th>\n",
       "      <th>av_change_abs_roll_mean_1000</th>\n",
       "      <th>av_change_rate_roll_mean_1000</th>\n",
       "      <th>abs_max_roll_mean_1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>0.027452</td>\n",
       "      <td>-0.087698</td>\n",
       "      <td>-0.269490</td>\n",
       "      <td>0.215853</td>\n",
       "      <td>0.719132</td>\n",
       "      <td>-0.096054</td>\n",
       "      <td>0.027452</td>\n",
       "      <td>-0.167696</td>\n",
       "      <td>-0.193129</td>\n",
       "      <td>-0.090150</td>\n",
       "      <td>-0.045555</td>\n",
       "      <td>0.045555</td>\n",
       "      <td>0.071087</td>\n",
       "      <td>-0.137971</td>\n",
       "      <td>-0.115036</td>\n",
       "      <td>0.210190</td>\n",
       "      <td>0.421946</td>\n",
       "      <td>-0.079699</td>\n",
       "      <td>-0.027912</td>\n",
       "      <td>-0.167696</td>\n",
       "      <td>-0.661836</td>\n",
       "      <td>0.134670</td>\n",
       "      <td>-0.262987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.083572</td>\n",
       "      <td>-0.055168</td>\n",
       "      <td>-0.185810</td>\n",
       "      <td>0.092718</td>\n",
       "      <td>-0.226750</td>\n",
       "      <td>0.275670</td>\n",
       "      <td>0.355614</td>\n",
       "      <td>0.753431</td>\n",
       "      <td>0.238715</td>\n",
       "      <td>0.059801</td>\n",
       "      <td>0.177342</td>\n",
       "      <td>-0.261028</td>\n",
       "      <td>-0.297410</td>\n",
       "      <td>-0.083764</td>\n",
       "      <td>-0.054257</td>\n",
       "      <td>0.343232</td>\n",
       "      <td>...</td>\n",
       "      <td>1.004377</td>\n",
       "      <td>0.873464</td>\n",
       "      <td>0.308992</td>\n",
       "      <td>-0.085567</td>\n",
       "      <td>-0.040546</td>\n",
       "      <td>-0.677462</td>\n",
       "      <td>-0.267826</td>\n",
       "      <td>0.027420</td>\n",
       "      <td>-0.080386</td>\n",
       "      <td>-0.204755</td>\n",
       "      <td>0.156511</td>\n",
       "      <td>0.039994</td>\n",
       "      <td>-0.175608</td>\n",
       "      <td>0.128090</td>\n",
       "      <td>-0.063237</td>\n",
       "      <td>0.605292</td>\n",
       "      <td>-0.666983</td>\n",
       "      <td>-0.203755</td>\n",
       "      <td>0.118937</td>\n",
       "      <td>-0.147112</td>\n",
       "      <td>-0.235083</td>\n",
       "      <td>1.661043</td>\n",
       "      <td>1.468996</td>\n",
       "      <td>0.967368</td>\n",
       "      <td>0.213094</td>\n",
       "      <td>-0.160542</td>\n",
       "      <td>-0.005777</td>\n",
       "      <td>-0.460986</td>\n",
       "      <td>-0.235083</td>\n",
       "      <td>0.027895</td>\n",
       "      <td>0.142369</td>\n",
       "      <td>-0.130344</td>\n",
       "      <td>0.042361</td>\n",
       "      <td>-0.101836</td>\n",
       "      <td>-0.347732</td>\n",
       "      <td>0.065341</td>\n",
       "      <td>-0.104108</td>\n",
       "      <td>1.351680</td>\n",
       "      <td>-0.460627</td>\n",
       "      <td>-0.130344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.115378</td>\n",
       "      <td>-0.131137</td>\n",
       "      <td>-0.166897</td>\n",
       "      <td>0.106453</td>\n",
       "      <td>0.399009</td>\n",
       "      <td>-0.126087</td>\n",
       "      <td>0.115378</td>\n",
       "      <td>0.261381</td>\n",
       "      <td>0.684865</td>\n",
       "      <td>-0.128834</td>\n",
       "      <td>-0.182753</td>\n",
       "      <td>0.182753</td>\n",
       "      <td>-0.163707</td>\n",
       "      <td>-0.060261</td>\n",
       "      <td>-0.060986</td>\n",
       "      <td>0.104549</td>\n",
       "      <td>0.117843</td>\n",
       "      <td>-0.131761</td>\n",
       "      <td>0.257551</td>\n",
       "      <td>0.261381</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.953452</td>\n",
       "      <td>-0.165305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.208579</td>\n",
       "      <td>0.101055</td>\n",
       "      <td>-0.306316</td>\n",
       "      <td>-0.206048</td>\n",
       "      <td>0.305227</td>\n",
       "      <td>-0.029501</td>\n",
       "      <td>0.180397</td>\n",
       "      <td>-0.200577</td>\n",
       "      <td>0.305966</td>\n",
       "      <td>-0.178674</td>\n",
       "      <td>0.359968</td>\n",
       "      <td>0.298090</td>\n",
       "      <td>-0.400516</td>\n",
       "      <td>0.063890</td>\n",
       "      <td>-0.482095</td>\n",
       "      <td>0.014077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674615</td>\n",
       "      <td>0.314595</td>\n",
       "      <td>-0.054737</td>\n",
       "      <td>-0.142397</td>\n",
       "      <td>-0.021793</td>\n",
       "      <td>1.395713</td>\n",
       "      <td>-0.134648</td>\n",
       "      <td>0.115703</td>\n",
       "      <td>-0.105863</td>\n",
       "      <td>-0.045825</td>\n",
       "      <td>0.059198</td>\n",
       "      <td>0.160208</td>\n",
       "      <td>0.244954</td>\n",
       "      <td>-0.036295</td>\n",
       "      <td>-0.069870</td>\n",
       "      <td>-0.151013</td>\n",
       "      <td>1.382276</td>\n",
       "      <td>-0.046856</td>\n",
       "      <td>-0.045959</td>\n",
       "      <td>-0.161019</td>\n",
       "      <td>-0.164371</td>\n",
       "      <td>-0.633780</td>\n",
       "      <td>0.281579</td>\n",
       "      <td>0.314599</td>\n",
       "      <td>-0.086557</td>\n",
       "      <td>-0.148539</td>\n",
       "      <td>-0.013052</td>\n",
       "      <td>2.077555</td>\n",
       "      <td>-0.164371</td>\n",
       "      <td>0.108406</td>\n",
       "      <td>-0.281902</td>\n",
       "      <td>-0.066430</td>\n",
       "      <td>0.153030</td>\n",
       "      <td>0.241548</td>\n",
       "      <td>0.417061</td>\n",
       "      <td>-0.134180</td>\n",
       "      <td>0.015884</td>\n",
       "      <td>-1.180161</td>\n",
       "      <td>2.078594</td>\n",
       "      <td>-0.066430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>-0.167016</td>\n",
       "      <td>-0.302077</td>\n",
       "      <td>-0.357426</td>\n",
       "      <td>0.336570</td>\n",
       "      <td>0.078886</td>\n",
       "      <td>-0.239161</td>\n",
       "      <td>-0.167016</td>\n",
       "      <td>0.369263</td>\n",
       "      <td>0.072311</td>\n",
       "      <td>-0.302973</td>\n",
       "      <td>-0.366208</td>\n",
       "      <td>0.366208</td>\n",
       "      <td>0.302925</td>\n",
       "      <td>-0.274209</td>\n",
       "      <td>-0.355799</td>\n",
       "      <td>0.145684</td>\n",
       "      <td>-0.220981</td>\n",
       "      <td>-0.317162</td>\n",
       "      <td>-0.369607</td>\n",
       "      <td>0.369263</td>\n",
       "      <td>-0.661836</td>\n",
       "      <td>1.041195</td>\n",
       "      <td>-0.350564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.212034</td>\n",
       "      <td>-0.231694</td>\n",
       "      <td>-0.237863</td>\n",
       "      <td>0.097413</td>\n",
       "      <td>-0.677718</td>\n",
       "      <td>-0.019305</td>\n",
       "      <td>-1.062462</td>\n",
       "      <td>-0.176132</td>\n",
       "      <td>0.189806</td>\n",
       "      <td>0.278939</td>\n",
       "      <td>0.096175</td>\n",
       "      <td>-0.121249</td>\n",
       "      <td>-0.276789</td>\n",
       "      <td>-0.210325</td>\n",
       "      <td>-0.164667</td>\n",
       "      <td>0.239289</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.154419</td>\n",
       "      <td>-1.132461</td>\n",
       "      <td>-0.619537</td>\n",
       "      <td>-0.257097</td>\n",
       "      <td>0.606997</td>\n",
       "      <td>0.409327</td>\n",
       "      <td>-0.350588</td>\n",
       "      <td>-0.166101</td>\n",
       "      <td>-0.162858</td>\n",
       "      <td>-0.268570</td>\n",
       "      <td>0.277694</td>\n",
       "      <td>0.134900</td>\n",
       "      <td>-0.046205</td>\n",
       "      <td>-0.299311</td>\n",
       "      <td>-0.215803</td>\n",
       "      <td>0.605292</td>\n",
       "      <td>0.415057</td>\n",
       "      <td>-0.266755</td>\n",
       "      <td>-0.522238</td>\n",
       "      <td>-0.294011</td>\n",
       "      <td>-0.272619</td>\n",
       "      <td>-0.426197</td>\n",
       "      <td>-0.953331</td>\n",
       "      <td>-0.971573</td>\n",
       "      <td>-0.563212</td>\n",
       "      <td>-0.249559</td>\n",
       "      <td>0.262861</td>\n",
       "      <td>-0.032062</td>\n",
       "      <td>-0.272619</td>\n",
       "      <td>-0.156390</td>\n",
       "      <td>0.023288</td>\n",
       "      <td>-0.167548</td>\n",
       "      <td>0.113896</td>\n",
       "      <td>-0.058133</td>\n",
       "      <td>-0.189001</td>\n",
       "      <td>-0.010840</td>\n",
       "      <td>-0.228628</td>\n",
       "      <td>0.860994</td>\n",
       "      <td>-0.030970</td>\n",
       "      <td>-0.167548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>0.186250</td>\n",
       "      <td>-0.138458</td>\n",
       "      <td>-0.163233</td>\n",
       "      <td>0.140404</td>\n",
       "      <td>-0.401299</td>\n",
       "      <td>-0.123944</td>\n",
       "      <td>0.186250</td>\n",
       "      <td>-0.045393</td>\n",
       "      <td>-0.499407</td>\n",
       "      <td>-0.143109</td>\n",
       "      <td>-0.104602</td>\n",
       "      <td>0.104602</td>\n",
       "      <td>-0.325291</td>\n",
       "      <td>-0.161837</td>\n",
       "      <td>-0.241799</td>\n",
       "      <td>0.015302</td>\n",
       "      <td>-0.650369</td>\n",
       "      <td>-0.132315</td>\n",
       "      <td>-0.101288</td>\n",
       "      <td>-0.045393</td>\n",
       "      <td>0.113511</td>\n",
       "      <td>0.401327</td>\n",
       "      <td>-0.172041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.097574</td>\n",
       "      <td>-0.110824</td>\n",
       "      <td>0.285064</td>\n",
       "      <td>-0.218314</td>\n",
       "      <td>-0.005786</td>\n",
       "      <td>-0.122364</td>\n",
       "      <td>-0.624094</td>\n",
       "      <td>-0.868640</td>\n",
       "      <td>0.177579</td>\n",
       "      <td>0.214487</td>\n",
       "      <td>-0.147325</td>\n",
       "      <td>0.360214</td>\n",
       "      <td>-0.187431</td>\n",
       "      <td>-0.152318</td>\n",
       "      <td>0.318376</td>\n",
       "      <td>-0.384374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633171</td>\n",
       "      <td>0.403044</td>\n",
       "      <td>0.027019</td>\n",
       "      <td>-0.113594</td>\n",
       "      <td>-0.040567</td>\n",
       "      <td>0.052206</td>\n",
       "      <td>-0.179361</td>\n",
       "      <td>0.186808</td>\n",
       "      <td>-0.113863</td>\n",
       "      <td>-0.172237</td>\n",
       "      <td>0.126216</td>\n",
       "      <td>0.109592</td>\n",
       "      <td>0.180252</td>\n",
       "      <td>0.128090</td>\n",
       "      <td>-0.043337</td>\n",
       "      <td>-0.725245</td>\n",
       "      <td>0.048813</td>\n",
       "      <td>-0.171653</td>\n",
       "      <td>-0.100563</td>\n",
       "      <td>-0.153301</td>\n",
       "      <td>-0.199554</td>\n",
       "      <td>0.922908</td>\n",
       "      <td>0.472134</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>-0.016877</td>\n",
       "      <td>-0.138393</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>0.599292</td>\n",
       "      <td>-0.199554</td>\n",
       "      <td>0.191584</td>\n",
       "      <td>-0.103449</td>\n",
       "      <td>-0.104588</td>\n",
       "      <td>0.123995</td>\n",
       "      <td>0.056329</td>\n",
       "      <td>0.175357</td>\n",
       "      <td>0.021809</td>\n",
       "      <td>0.027204</td>\n",
       "      <td>-1.228218</td>\n",
       "      <td>0.603304</td>\n",
       "      <td>-0.104588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>-1.489370</td>\n",
       "      <td>-0.079348</td>\n",
       "      <td>-0.119265</td>\n",
       "      <td>0.196991</td>\n",
       "      <td>0.879193</td>\n",
       "      <td>-0.138849</td>\n",
       "      <td>-1.489370</td>\n",
       "      <td>0.082082</td>\n",
       "      <td>-0.070618</td>\n",
       "      <td>-0.081356</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>-0.011428</td>\n",
       "      <td>0.329455</td>\n",
       "      <td>-0.039953</td>\n",
       "      <td>0.045709</td>\n",
       "      <td>0.023934</td>\n",
       "      <td>0.309362</td>\n",
       "      <td>-0.070913</td>\n",
       "      <td>-0.042531</td>\n",
       "      <td>0.082082</td>\n",
       "      <td>-0.551073</td>\n",
       "      <td>-1.012591</td>\n",
       "      <td>-0.131621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.268188</td>\n",
       "      <td>-0.259669</td>\n",
       "      <td>-1.685328</td>\n",
       "      <td>-0.997310</td>\n",
       "      <td>-1.821515</td>\n",
       "      <td>-1.153942</td>\n",
       "      <td>0.085874</td>\n",
       "      <td>-0.023988</td>\n",
       "      <td>-0.269076</td>\n",
       "      <td>0.329152</td>\n",
       "      <td>-0.139315</td>\n",
       "      <td>0.132444</td>\n",
       "      <td>0.414985</td>\n",
       "      <td>-0.419022</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078211</td>\n",
       "      <td>-0.136780</td>\n",
       "      <td>-0.015096</td>\n",
       "      <td>-0.004439</td>\n",
       "      <td>-0.118214</td>\n",
       "      <td>-0.286575</td>\n",
       "      <td>-0.067199</td>\n",
       "      <td>-1.489933</td>\n",
       "      <td>-0.111823</td>\n",
       "      <td>-0.148662</td>\n",
       "      <td>0.131265</td>\n",
       "      <td>-0.124509</td>\n",
       "      <td>-0.984381</td>\n",
       "      <td>-1.482885</td>\n",
       "      <td>-0.361737</td>\n",
       "      <td>-0.319081</td>\n",
       "      <td>-0.341950</td>\n",
       "      <td>-0.148379</td>\n",
       "      <td>-0.074931</td>\n",
       "      <td>-0.064313</td>\n",
       "      <td>-0.078706</td>\n",
       "      <td>-0.434966</td>\n",
       "      <td>-0.690496</td>\n",
       "      <td>-0.267298</td>\n",
       "      <td>0.178456</td>\n",
       "      <td>-0.097583</td>\n",
       "      <td>0.222101</td>\n",
       "      <td>0.091790</td>\n",
       "      <td>-0.078706</td>\n",
       "      <td>-1.488654</td>\n",
       "      <td>-0.360152</td>\n",
       "      <td>-0.232734</td>\n",
       "      <td>-0.038432</td>\n",
       "      <td>-0.499329</td>\n",
       "      <td>-1.047589</td>\n",
       "      <td>-1.636035</td>\n",
       "      <td>-1.120646</td>\n",
       "      <td>0.251430</td>\n",
       "      <td>0.083907</td>\n",
       "      <td>-0.232734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean           ...            abs_max_roll_mean_1000\n",
       "3057  0.027452           ...                         -0.130344\n",
       "261   0.115378           ...                         -0.066430\n",
       "2577 -0.167016           ...                         -0.167548\n",
       "2994  0.186250           ...                         -0.104588\n",
       "1356 -1.489370           ...                         -0.232734\n",
       "\n",
       "[5 rows x 154 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "54c5f421cdff29b9b4d607819b2cf4afedd2f0f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>0.807699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>2.802197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>8.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>3.262299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>3.923996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      time_to_failure\n",
       "3057         0.807699\n",
       "261          2.802197\n",
       "2577         8.485600\n",
       "2994         3.262299\n",
       "1356         3.923996"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "3a14adfa4cf698bdc397b9faca5bd773464c3163"
   },
   "outputs": [],
   "source": [
    "trainlen = int(X.shape[0] * 0.8)\n",
    "X_train = X.iloc[:trainlen,:]\n",
    "X_val = X.iloc[trainlen:,:]\n",
    "Y_train = Y.iloc[:trainlen,:]\n",
    "Y_val = Y.iloc[trainlen:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "acd9d71620047845284e4687cb8013ed1b57cdcf"
   },
   "outputs": [],
   "source": [
    "epochs = 12\n",
    "batch_size = 32\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=20, min_lr=1e-11)\n",
    "logging = TensorBoard(\"output\")\n",
    "checkpoint = ModelCheckpoint(\"output/best_weights.h5\", save_best_only = True, save_weights_only = True, mode = \"min\", monitor = \"val_loss\", verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "d75dc064064a038b92c74563f97533d626e1cc51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3355 samples, validate on 839 samples\n",
      "Epoch 1/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 5.7208 - val_loss: 4.5371\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.53712, saving model to output/best_weights.h5\n",
      "Epoch 2/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 5.5750 - val_loss: 4.4180\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.53712 to 4.41800, saving model to output/best_weights.h5\n",
      "Epoch 3/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 5.4358 - val_loss: 4.3318\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.41800 to 4.33180, saving model to output/best_weights.h5\n",
      "Epoch 4/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 5.3152 - val_loss: 4.2083\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.33180 to 4.20827, saving model to output/best_weights.h5\n",
      "Epoch 5/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 5.1537 - val_loss: 4.1193\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.20827 to 4.11925, saving model to output/best_weights.h5\n",
      "Epoch 6/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 5.0112 - val_loss: 3.9374\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.11925 to 3.93739, saving model to output/best_weights.h5\n",
      "Epoch 7/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 4.8772 - val_loss: 3.8466\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.93739 to 3.84657, saving model to output/best_weights.h5\n",
      "Epoch 8/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 4.7433 - val_loss: 3.7536\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.84657 to 3.75356, saving model to output/best_weights.h5\n",
      "Epoch 9/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 4.5927 - val_loss: 3.6481\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.75356 to 3.64808, saving model to output/best_weights.h5\n",
      "Epoch 10/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 4.4298 - val_loss: 3.5423\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.64808 to 3.54229, saving model to output/best_weights.h5\n",
      "Epoch 11/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 4.2773 - val_loss: 3.4288\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.54229 to 3.42876, saving model to output/best_weights.h5\n",
      "Epoch 12/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 4.1453 - val_loss: 3.2404\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.42876 to 3.24045, saving model to output/best_weights.h5\n",
      "Epoch 13/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 3.9924 - val_loss: 3.0947\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.24045 to 3.09469, saving model to output/best_weights.h5\n",
      "Epoch 14/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 3.8273 - val_loss: 2.9657\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.09469 to 2.96574, saving model to output/best_weights.h5\n",
      "Epoch 15/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 3.6808 - val_loss: 2.8744\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.96574 to 2.87436, saving model to output/best_weights.h5\n",
      "Epoch 16/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 3.5144 - val_loss: 2.8039\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.87436 to 2.80385, saving model to output/best_weights.h5\n",
      "Epoch 17/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 3.4102 - val_loss: 2.7224\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.80385 to 2.72235, saving model to output/best_weights.h5\n",
      "Epoch 18/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 3.2517 - val_loss: 2.5734\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.72235 to 2.57337, saving model to output/best_weights.h5\n",
      "Epoch 19/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 3.1425 - val_loss: 2.5347\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.57337 to 2.53466, saving model to output/best_weights.h5\n",
      "Epoch 20/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 3.0062 - val_loss: 2.4523\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.53466 to 2.45228, saving model to output/best_weights.h5\n",
      "Epoch 21/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 2.9150 - val_loss: 2.3936\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.45228 to 2.39361, saving model to output/best_weights.h5\n",
      "Epoch 22/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.7961 - val_loss: 2.3673\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.39361 to 2.36731, saving model to output/best_weights.h5\n",
      "Epoch 23/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.6875 - val_loss: 2.3329\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.36731 to 2.33287, saving model to output/best_weights.h5\n",
      "Epoch 24/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 2.6641 - val_loss: 2.3103\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.33287 to 2.31030, saving model to output/best_weights.h5\n",
      "Epoch 25/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 2.6158 - val_loss: 2.2937\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.31030 to 2.29374, saving model to output/best_weights.h5\n",
      "Epoch 26/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.5226 - val_loss: 2.2775\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.29374 to 2.27753, saving model to output/best_weights.h5\n",
      "Epoch 27/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.4781 - val_loss: 2.2723\n",
      "\n",
      "Epoch 00027: val_loss improved from 2.27753 to 2.27234, saving model to output/best_weights.h5\n",
      "Epoch 28/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.4038 - val_loss: 2.3273\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.27234\n",
      "Epoch 29/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.3908 - val_loss: 2.3358\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.27234\n",
      "Epoch 30/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.3407 - val_loss: 2.3279\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.27234\n",
      "Epoch 31/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.3343 - val_loss: 2.2975\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.27234\n",
      "Epoch 32/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 2.3194 - val_loss: 2.3247\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.27234\n",
      "Epoch 33/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.3312 - val_loss: 2.3023\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.27234\n",
      "Epoch 34/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.2658 - val_loss: 2.2559\n",
      "\n",
      "Epoch 00034: val_loss improved from 2.27234 to 2.25594, saving model to output/best_weights.h5\n",
      "Epoch 35/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.2538 - val_loss: 2.2663\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.25594\n",
      "Epoch 36/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.2581 - val_loss: 2.2385\n",
      "\n",
      "Epoch 00036: val_loss improved from 2.25594 to 2.23848, saving model to output/best_weights.h5\n",
      "Epoch 37/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 2.2153 - val_loss: 2.2542\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.23848\n",
      "Epoch 38/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.1719 - val_loss: 2.3120\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.23848\n",
      "Epoch 39/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.1581 - val_loss: 2.2670\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.23848\n",
      "Epoch 40/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.1897 - val_loss: 2.2534\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.23848\n",
      "Epoch 41/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.1701 - val_loss: 2.2883\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.23848\n",
      "Epoch 42/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.1416 - val_loss: 2.3276\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.23848\n",
      "Epoch 43/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 2.1354 - val_loss: 2.2619\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.23848\n",
      "Epoch 44/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.1374 - val_loss: 2.2701\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.23848\n",
      "Epoch 45/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.1223 - val_loss: 2.2868\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.23848\n",
      "Epoch 46/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.0882 - val_loss: 2.3441\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.23848\n",
      "Epoch 47/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.0728 - val_loss: 2.3130\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.23848\n",
      "Epoch 48/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.0837 - val_loss: 2.2932\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.23848\n",
      "Epoch 49/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 2.0692 - val_loss: 2.3433\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.23848\n",
      "Epoch 50/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 2.0920 - val_loss: 2.3127\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.23848\n",
      "Epoch 51/1000\n",
      "3355/3355 [==============================] - 13s 4ms/step - loss: 2.0551 - val_loss: 2.3181\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.23848\n",
      "Epoch 52/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.0180 - val_loss: 2.2596\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.23848\n",
      "Epoch 53/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.0494 - val_loss: 2.3352\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.23848\n",
      "Epoch 54/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.0483 - val_loss: 2.3364\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.23848\n",
      "Epoch 55/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.0029 - val_loss: 2.2930\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.23848\n",
      "Epoch 56/1000\n",
      "3355/3355 [==============================] - 12s 4ms/step - loss: 2.0403 - val_loss: 2.3187\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.23848\n",
      "Epoch 57/1000\n",
      " 736/3355 [=====>........................] - ETA: 9s - loss: 1.9281"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,epochs = 1000, batch_size = batch_size, validation_data = (X_val,Y_val), callbacks = [logging,checkpoint], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "43aa184d2fd02bd9f02952b37d5016a92fdf05c2"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "7af6710e11d7b4e00fcb9251eb885046a5a56fb4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "8fae560df8462b7c851fc99dfe6e46029441af75"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
